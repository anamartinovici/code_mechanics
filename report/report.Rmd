---
title             : "EEG Many Pipelines - Code Mechanics"
shorttitle        : "Code Mechanics"
author:
  - name          : "Sebastian Speer"
    affiliation   : "1"
  - name          : "Antonio Schettino"
    affiliation   : "2,3"
  - name          : "Ana Martinovici"
    affiliation   : "4"
    corresponding : yes # only one corresponding author
    address       : "Burgemeester Oudlaan 50, 3062 PA Rotterdam, Netherlands"
    email         : "martinovici@rsm.nl"
affiliation:
  - id            : "1"
    institution   : "Social Brain Lab, Netherlands Institute for Neuroscience, Amsterdam, The Netherlands"
  - id            : "2"
    institution   : "Erasmus Research Services, Erasmus University Rotterdam, Rotterdam, The Netherlands"    
  - id            : "3"
    institution   : "Institute for Globally Distributed Open Research and Education (IGDORE), Sweden"
  - id            : "4"
    institution   : "Rotterdam School or Management, Erasmus University Rotterdam, Rotterdam, The Netherlands"
authornote: |
  Authorship order was randomly determined via the `sample` function in *R*. 
  
  **SS** preprocessed the data and performed time-frequency analysis (research questions 2b, 2c, 3b, 4b). **AS** preprocessed the data and performed ERP analysis (research questions 1, 2a, 3a, 4a). **AM** was responsible for project management, GitHub repository, and reproducibility. **SS**, **AS**, and **AM** wrote the report.
abstract: |
  <!-- https://tinyurl.com/ybremelq -->
  
  One or two sentences providing a **basic introduction** to the field, comprehensible to a scientist in any discipline.
  
  Two to three sentences of **more detailed background**, comprehensible to scientists in related disciplines.
  
  One sentence clearly stating the **general problem** being addressed by this particular study.
  
  One sentence summarizing the main result (with the words "**here we show**" or their equivalent).
  
  Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the main result adds to previous knowledge.
  
  One or two sentences to put the results into a more **general context**.
  
  Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.

keywords          : ["EEG Many Pipelines", "scene categorization", "vision", "EEG", "ERP", "time-frequency analysis", "Bayesian multilevel linear regression", "TFCE"]
wordcount         : "X" # to be added when the report is completed 
bibliography      : ["references.bib"] # "r-references.bib": separate reference file for R/RStudio/packages, but does not correctly visualize some references (e.g., RStudio, brms)
link-citations    : true # create hyperlink to the corresponding bibliography entry
zotero            : "EEGManyPipelines_CodeMechanics"
floatsintext      : yes # place figures and tables in the text rather than at the end (default)?
linenumbers       : no # add line numbers in the margins?
draft             : no # add “DRAFT” watermark to every page?
mask              : no # omit identifying information from the title page?
figurelist        : no # list of figure captions after the reference section?
tablelist         : no # list of table captions after the reference section?
footnotelist      : no # list of footnotes after the reference section?
numbersections    : yes # are section headers numbered? 
classoption       : "man" # option "doc" produces a non-APA-formatted document (single-spaced, single-column)
output: 
  papaja::apa6_pdf:
    keep_tex: FALSE # delete the LaTeX source file after the PDF has been rendered
editor_options: 
  chunk_output_type: console
---

```{r install-latex, include = FALSE, eval = FALSE}

# if LaTeX is not installed:
install.packages("tinytex")
library("tinytex")
tinytex::install_tinytex()

```

```{r install-packages, include = FALSE, eval = FALSE}

devtools::install_github("crsh/papaja") # render document in APA format
install.packages("here")

```

```{r setup, include = FALSE}

# load packages
library("here")
library("papaja")

# # load bibliography file of R packages
# # commented out because it does not correctly visualize 
# # some references (e.g., RStudio, brms)
# r_refs("r-references.bib")

# knitr global chunk options
knitr::opts_chunk$set(
	# cache.extra = knitr::rand_seed # found in papaja template, perhaps not necessary?
	out.width = '90%' # plot size in the output document
	)

# set seed for random number generation
seed_project <- 999
set.seed(seed_project)

```

```{r authorship-order, include = FALSE, eval = FALSE}

# procedure used to determine authorship order
authors <- c("Ana Martinovici", "Antonio Schettino", "Sebastian Speer") # initial order is alphabetical
authorship_order <- sample(authors)

authorship_order

```

# Introduction

Explain how we got the data and instructions.

# Methods

## Preprocessing - TFR

EEG data were filtered with a low cutoff filter of 1 Hz to remove slow drifts, a high cutoff filter at 40 Hz to attenuate high frequency power. Subsequently, bad and noisy channels were detected using several different approaches as implemented in the PREP pipeline [@bigdely-shamlo2015]. First, by means of correlation, we checked how well a given channel is correlated with all other channels (categorized as bad at *r* , 0.4); second, we checked by using the robust *z*-score deviation aggregates per channel (categorized as bad at *z* . 5); third, by using the robust *z*-score estimates of high-frequency noise per channel (categorized as bad at *z* . 5); and finally, we checked by using the random sample consensus (RANSAC) channel correlations, which is the correlation for each channel with itself across the original data versus the RANSAC predicted data (categorized as bad at *r* , 0.75) as implemented in the PREP pipeline [@bigdely-shamlo2015]. After detection, these channels were removed from the data and subsequently interpolated (i.e., estimated from surrounding channels). Interpolation was performed using the spherical spline method [@perrin1989] as implemented in MNE-Python, which projects the sensor locations onto a unit sphere and interpolates the signal at the channels identified as bad on the signals for the good channels. Afterwards, the EEG data were re-referenced to the average signal across channels. Next, ocular artifacts were removed by performing an independent component analysis on the data and then correlating the resulting components with the EOG channels to see which of the components represented the ocular artifacts. The component that correlated the highest with the EOG channels was then removed from the EEG data.

### Cutoffs

```{r}
file_name <- here("scripts", "TFR_preproc_step1.py")
start_line <- 30
end_line <- 32
knitr::read_chunk(path = file_name, labels = "ex-1", from = start_line, to = end_line)

file_in_repo <- stringr::str_sub(file_name, start = stringr::str_locate(file_name, "/code_mechanics")[1, "start"])

```

Implemented in file: `r file_in_repo`, lines `r start_line`:`r end_line`.

```{r, ex-1, eval = FALSE, echo = TRUE}
```

### Bad Channels at 0.4

```{r}
file_name <- here("scripts", "TFR_preproc_step1.py")
start_line <- 279
end_line <- 298
knitr::read_chunk(path = file_name, labels = "ex-2", from = start_line, to = end_line)

file_in_repo <- stringr::str_sub(file_name, start = stringr::str_locate(file_name, "/code_mechanics")[1, "start"])

```

Implemented in file: `r file_in_repo`, lines `r start_line`:`r end_line`.

```{r, ex-2, eval = FALSE, echo = TRUE}
```

### Questions for Sebastian

-   Where do you check the "robust *z*-score deviation aggregates per channel (categorized as bad at *z* . 5)"?

-   Is the "do not apply 1 Hz high-pass filter" comment accurate? If so, isn't that what happens at lines 242:247?

```{r}
file_name <- here("scripts", "TFR_preproc_step1.py")
start_line <- 241
end_line <- 247
knitr::read_chunk(path = file_name, labels = "ex-3", from = start_line, to = end_line)

file_in_repo <- stringr::str_sub(file_name, start = stringr::str_locate(file_name, "/code_mechanics")[1, "start"])

```

Implemented in file: `r file_in_repo`, lines `r start_line`:`r end_line`.

```{r, ex-3, eval = FALSE, echo = TRUE}
```

```{r}
file_name <- here("scripts", "TFR_preproc_step1.py")
start_line <- 269
end_line <- 277
knitr::read_chunk(path = file_name, labels = "ex-4", from = start_line, to = end_line)

file_in_repo <- stringr::str_sub(file_name, start = stringr::str_locate(file_name, "/code_mechanics")[1, "start"])

```

Implemented in file: `r file_in_repo`, lines `r start_line`:`r end_line`.

```{r, ex-4, eval = FALSE, echo = TRUE}
```

## Epoching

The EEG data was then subsampled by a factor of four (i.e., from 512 Hz to 128 Hz and segmented into 800 ms epochs, with 300 ms before stimulus onset (onset of the scene) until 500 ms after stimulus onset. The epochs were baseline corrected using the 300 ms preceding the stimulus onset. The resulting epochs were then subjected to *Autoreject*, an automated artifact detection algorithm based on machine-learning classifiers, and cross-validation to estimate the optimal peak-to-peak threshold [@jas2017]. This algorithm was implemented to remove artifacts not identified by previous preprocessing steps, and depending on the number of bad sensors for a given trial, either repairs the trial based on interpolation or excludes it from further analysis. The preprocessed data were then submitted to a Morlet wavelet analysis to transform the data into the time-frequency domain with 18 log-scaled frequency bins ranging from 4 to 40 Hz to have higher sensitivity in lower frequency ranges such as the theta band. To optimize both spectral and temporal resolution, the number of cycles to include in the sliding time window were defined by dividing each individual frequency by two. After transforming the data to the time-frequency domain, the data were decimated by a factor of two (sampling every second time point) to increase computational efficiency.

### downsample and segmentation

```{r}
file_name <- here("scripts", "TFR_preproc_step1.py")
start_line <- 413
end_line <- 427
knitr::read_chunk(path = file_name, labels = "ex-5", from = start_line, to = end_line)

file_in_repo <- stringr::str_sub(file_name, start = stringr::str_locate(file_name, "/code_mechanics")[1, "start"])

```

Implemented in file: `r file_in_repo`, lines `r start_line`:`r end_line`.

```{r, ex-5, eval = FALSE, echo = TRUE}
```

# Analysis

## RQ1

The first hypothesis we were asked to test was the following:

> *There is an effect of scene category (i.e., a difference between images showing man-made vs. natural environments) on the amplitude of the N1 component, i.e. the first major negative EEG voltage deflection.*

To address this question, we fit a Bayesian multilevel linear model on N1 amplitude values, with *condition* (2 levels: *man-made*, *natural*) as *constant* (a.k.a. fixed) effect and participant and trial as *varying* (a.k.a. random) effects. We allowed intercepts and slopes to vary as a function of participant and trials, to model general and condition-specific inter-individual differences. As a likelihood function, we chose a Gaussian distribution.\
An important aspect of Bayesian analysis is the choice of priors [e.g., @natarajan2000]. Given the susceptibility of the electrophysiological signal to inter-individual differences (e.g., skull thickness, skin conductance, or hair), we decided to base our priors on the current data by visually inspecting the grand average ERP waveform, i.e., collapsed across *man-made* and *natural* condition. This procedure follows the same logic of *collapsed localizers*, useful when prior research does not allow to reliably identify scalp electrodes and time windows related to ERP components of interest [e.g., @Luck2017, p. 150]. For the main analysis, we placed **informative priors** on the *intercept* -- a normal distribution with mean $\mu$ = 4 and standard deviation $\sigma$ = 2: $Normal(4,2)$ -- and $\beta$ coefficient, $Normal(0,1)$. To assess whether our chosen informative prior would bias parameter estimates (and, consequently, the interpretation of the results; @depaoli2017), we ran the same multilevel linear model with **weakly informative** priors (*intercept*: $Normal(4,4)$ ; $\beta$: $Normal(0,4)$) and **uninformative** priors (*intercept*: $Normal(4,10)$ ; $\beta$: $Normal(0,10)$). We anticipated that the choice of prior would have negligible effects on the posterior distributions, because the influence of the prior washes out with a large amount of data [@edwards1963].\
Since we had no prior knowledge regarding the standard deviation of participant and trials, we placed a **weakly informative prior** on these varying effects: a *t*-distribution with degrees of freedom $\nu$ = 3, location $\mu$ = 0, and scale $\sigma$ = 2, $Student(3,0,2)$.\
Models were fitted in *R* using the `brms` package [@bürkner2018], which employs the probabilistic programming language *Stan* [@carpenter2017] to implement a Markov chain Monte Carlo (MCMC) algorithm (i.e., No-U-Turn sampler; @homan2014) to estimate posterior distributions of the parameters of interest. Four MCMC chains with 4000 iterations (2000 warm-up) and no thinning were run to estimate parameters in each of the fitted models. Model convergence was assessed as follows: (*i*) visual inspection of trace plots, rank plots, and graphical posterior predictive checks [@gabry2019]; (*ii*) Gelman-Rubin $\hat{R}$ statistic [@gelman2013] -- comparing the between-chains variability to the within-chain variability -- between 1 and 1.05 [see also @Nalborczyk2019].\
Posterior distributions of the model parameters were summarized using the mean and 95% credible interval (CI). Differences between conditions were calculated by computing the difference between posterior distributions of the respective conditions.\
Statistical inference was performed using the **HDI + ROPE** decision rule [@kruschke2018]: values were accepted or rejected against a null hypothesis considering a small effect as practically equivalent to zero (Region of Practical Equivalence; *ROPE*). To mitigate the inevitable subjectivity intrinsic in arbitrarily choosing the range of negligible values, we explored a range of plausible ROPEs, from ±0.05$\mu$*V* to ±0.5$\mu$*V* in steps of 0.01$\mu$*V*. If the percentage of the posterior differences within the full ROPE was smaller than 5%, the null hypothesis was rejected.

## RQ2

To test whether there are effects of image novelty (RQ2; i.e., between images shown for the first time/new vs. repeated/old images) we conducted a multilevel analysis contrasting the EEG data from trials with old images against trials with new images. To test for differences in theta power at fronto-central channels we focused on the frequency range from 4-8 Hz and all frontocentral channels (FC1, FCz, FC2). At the first level (i.e., the participant level), we computed the averaged time-frequency maps for each of the two conditions. We then tested the resulting averaged maps at the second level for significant group effects, using a paired-sample *t*-test. We used cluster-based permutation testing as a stringent control for multiple comparisons [@maris2007]. Specifically, for every sample across the three channels, we quantified the experimental effect by a *t* value. Selection of samples for inclusion in a cluster was implemented using threshold-free cluster enhancement (TFCE) [@smith2009]. TFCE eliminates the free parameter initial threshold value that determines which points are included in clustering by approximating a continuous integration across possible threshold values with a standard Riemann sum. We subsequently clustered selected samples in connected sets based on temporal and spectral adjacency, and we computed cluster-level statistics by taking the sum of the *t* values within every cluster. Subsequently, we performed permutation testing using the Monte Carlo method to compute the posterior significance probability of our observed effect[@maris2007]. This analysis results in a cluster of adjacent data points across time, frequencies, and channels, which significantly differs in activity between old and new images. To test for differences in alpha power at posterior channels we focused on the frequency range from 8-13 Hz and all posterior channels (P7, P5, P3, P1, P2, P4, P6).

## RQ3

The same analysis approach as described for RQ2 was implemented for RQ3. Specifically, to test whether there are effects of successful recognition of old images on spectral power, at any frequencies, at any channel, at any time, we contrasted time-frequency decomposed EEG data from trials containing old images that were correctly recognized as old with old images incorrectly recognized as new. Here we included all frequencies, timepoints, and channels in the analysis. The same thresholding procedure and permutation testing approach as above was used.

## RQ4

To test whether there are effects of subsequent memory, we conducted exactly the same analysis as described in RQ3, with the only difference that here we contrasted trial containing images that will be successfully remembered vs. forgotten on a subsequent repetition.

# Results

(ref:figure01-caption) My caption.

```{r figure01, fig.cap = "(ref:figure01-caption)"}

knitr::include_graphics(here("results_in_repo/RQ1/timeseries_grand_average_ROI.png"))

```

No significant clusters were identified for any of the research questions (at $\alpha$ = 0.05).

# Discussion

ADD DISCUSSION HERE

# Software

EEG preprocessing was carried out using `MNE-Python` [@gramfort2013] in *Python* *v*3.9.7 [@vanrossum-2009] and *Spyder IDE* *v*5.1.5 [@raybaut2009]. Analysis, visualization, and report generation were carried out in *R* *v*4.1.3 [@R-base] and *RStudio IDE* *v*2022.02.1+461 [@R-studio]. We used the following *R* packages:

-   **data wrangling and analysis**: `here` *v*1.0.1 [@here], `Rmisc` *v*1.5 [@Rmisc], `tidyverse` *v*1.3.1 [@tidyverse] -- in particular `tibble` *v*3.1.6 [@tibble], `tidyr` *v*1.2.0 [@tidyr], `readr` *v*2.1.2 [@readr], `dplyr` *v*1.0.9 [@dplyr] --, `brms` *v*2.17.0 [@bürkner2018], `eegUtils` *v*0.7.0 [@eegUtils], `emmeans` *v*1.7.3 [@emmeans], `bayestestR` *v*0.11.5.1 [@bayestestR]

-   **visualization**: `ggplot2` *v*3.3.6 [@ggplot2], `eegUtils` *v*0.7.0 [@eegUtils], `bayesplot` *v*1.9.0 [@bayesplot], `viridis` *v*0.6.2 [@garnier2021], `tidybayes` *v*3.0.2 [@tidybayes], `patchwork` *v*1.1.1 [@patchwork]

-   **report generation**: `knitr` *v*1.39 [@knitr], `rmarkdown` *v*2.14 [@rmarkdown], `papaja` *v*0.1.0.9999 [@papaja]

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
