---
title             : "EEG Many Pipelines - The Code Mechanics"
shorttitle        : "Code Mechanics"
author:
  - name          : "Sebastian Speer"
    affiliation   : "1"
  - name          : "Antonio Schettino"
    affiliation   : "2,3"
  - name          : "Ana Martinovici"
    affiliation   : "4"
    corresponding : yes # only one corresponding author
    address       : "Burgemeester Oudlaan 50, 3062 PA Rotterdam, Netherlands"
    email         : "martinovici@rsm.nl"
affiliation:
  - id            : "1"
    institution   : "Social Brain Lab, Netherlands Institute for Neuroscience, Amsterdam, The Netherlands"
  - id            : "2"
    institution   : "Erasmus Research Services, Erasmus University Rotterdam, Rotterdam, The Netherlands"    
  - id            : "3"
    institution   : "Institute for Globally Distributed Open Research and Education (IGDORE), Sweden"
  - id            : "4"
    institution   : "Rotterdam School or Management, Erasmus University Rotterdam, Rotterdam, The Netherlands"
authornote: |
  Authorship order was randomly determined via the `sample` function in *R*. 
  
  **AS** preprocessed the data and performed ERP analysis (research questions 1, 2a, 3a, 4a). **SS** preprocessed the data and performed time-frequency analysis (research questions 2b, 2c, 3b, 4b). **AM** was responsible for project management, GitHub repository, and reproducibility. **AS**, **SS**, and **AM** wrote the report.
  
keywords          : ["EEGManyPipelines", "scene categorization", "vision", "EEG", "ERP", "time-frequency analysis", "Bayesian multilevel linear regression", "TFCE"]
wordcount         : "3915" # to be added when the report is completed 
bibliography      : "references.bib"
link-citations    : true # create hyperlink to the corresponding bibliography entry
zotero            : "EEGManyPipelines_CodeMechanics" # Zotero group library 
floatsintext      : yes # place figures and tables in the text rather than at the end?
linenumbers       : no # add line numbers in the margins?
draft             : no # add “DRAFT” watermark to every page?
mask              : no # omit identifying information from the title page?
figurelist        : no # list of figure captions after the reference section?
tablelist         : no # list of table captions after the reference section?
footnotelist      : no # list of footnotes after the reference section?
numbersections    : yes # are section headers numbered? 
classoption       : "man" # option "doc" produces a non-APA-formatted document (single-spaced, single-column)
output: 
  papaja::apa6_pdf:
    keep_tex: FALSE # delete the LaTeX source file after the PDF has been rendered
editor_options: 
  chunk_output_type: console
---

```{r install-latex, include = FALSE, eval = FALSE}

# if LaTeX is not installed:
install.packages("tinytex")
library("tinytex")
tinytex::install_tinytex()

```

```{r install-packages, include = FALSE, eval = FALSE}

install.packages("here")
install.packages("tidyverse")
install.packages("devtools")
devtools::install_github("crsh/papaja")

```

```{r setup, include = FALSE}

# load packages
library("here")
library("tidyverse")
library("papaja")

# knitr global chunk options
knitr::opts_chunk$set(
	out.width = '90%' # plot size in the output document
	)

# set seed for random number generation
seed_project <- 999
set.seed(seed_project)

```

```{r authorship-order, include = FALSE, eval = FALSE}

# procedure used to determine authorship order
authors <- c("Ana Martinovici", "Antonio Schettino", "Sebastian Speer") # initial order is alphabetical
authorship_order <- sample(authors)

authorship_order

```

# Introduction

This report describes the methods and results of our contribution to the [**EEGManyPipelines**](https://www.eegmanypipelines.org) project.\
We registered our team as **The Code Mechanics**[^1] on September 15th 2021 and received a confirmation email five days later. Instructions on how to download the EEG data and which research hypotheses to test were sent on October 8th 2021. A copy can be found at `data_in_repo/original_data/instructions`.\
We received the link to the submission portal on May 2nd 2022, submitted all relevant materials and filled out three questionnaires before the deadline (May 15th 2022).

[^1]: A homage to [**The Organic Mechanic**](https://madmax.fandom.com/wiki/The_Organic_Mechanic), a doctor in the movie [**Mad Max: Fury Road**](https://en.wikipedia.org/wiki/Mad_Max:_Fury_Road).

# Methods

## Preprocessing

### ERP

For each participant, the continuous EEG data was assigned electrode coordinates and filtered with two consecutive one-pass, zero-phase, non-causal FIR filters (Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation): (*i*) high-pass: 0.1 Hz (-6 dB cutoff frequency: 0.05 Hz), length 16897 samples; (*ii*) low-pass: 40 Hz, upper transition bandwidth 10 Hz (-6 dB cutoff frequency: 45.00 Hz), length 169 samples [@widmann2015]. Bad or noisy channels were detected using several approaches implemented in the *PREP* pipeline [@bigdely-shamlo2015], including: (*i*) flat or missing signal; (*ii*) "bad-by-high-frequency-noise" (frequency components above 50 Hz considerably higher than median channel noisiness, as calculated using a robust *Z*-scoring method and default threshold *z* \> 5); (*iii*) "bad-by-correlation" (maximum correlation with another channel below the default *r* = .4, fraction of bad correlation windows above the default threshold of 0.01); (*iv*) "bad-by-deviation" (amplitude deviates from the median channel amplitude, as calculated using a robust *Z*-scoring method and default threshold *z* \> 5); and (*v*) signal prediction based on signals and spatial locations of other channels lower than default threshold of *r* = .75 (random sample consensus approach, *RANSAC*; @fischler1987).[^2] Channels identified as noisy were removed from the data and subsequently interpolated via a spherical spline procedure [@perrin1989].\

[^2]: For more details, see the documentation of [`NoisyChannels`](https://pyprep.readthedocs.io/en/latest/generated/pyprep.NoisyChannels.html#pyprep.NoisyChannels).

Afterwards, the EEG data were re-referenced to the average signal across channels. Ocular artifacts were then corrected by means of independent component analysis via the `picard` algorithm [@ablin2018] and the resulting components were correlated with activity recorded from the EOG channels, in order to identify which components best represented horizontal eye movements and blinks. The components that correlated the highest with the EOG channels were then removed from the EEG data before re-conversion into channel space.\
The EEG data was subsampled by a factor of four (i.e., from 512 Hz to 128 Hz), segmented into epochs extending from -200ms to +500ms time-locked to scene onset, and baseline corrected using the pre-stimulus interval. The epoched data was then subjected to *Autoreject*, an automated artifact detection algorithm based on machine-learning classifiers and cross-validation to estimate the optimal peak-to-peak threshold [@jas2017] [^3]. This algorithm was implemented to remove artifacts not identified in previous preprocessing steps.\

[^3]: For more details, see the documentation of [`autoreject`](https://autoreject.github.io/stable/index.html).

### TFR

Signal preprocessing for time-frequency analysis followed a similar procedure, with the following exceptions: (*i*) high-pass FIR filter: 1 Hz (-6 dB cutoff frequency: 0.50 Hz), length 1691 samples; (*ii*) continuous data segmented into 800 ms epochs (-300ms to +500ms time-locked to scene onset) and baseline corrected using the pre-stimulus interval.\
The preprocessed data were then submitted to a Morlet wavelet analysis to transform the data into the time-frequency domain with 18 log-scaled frequency bins ranging from 4 to 40 Hz, to increase sensitivity at lower frequency ranges. To optimize both spectral and temporal resolution, the number of cycles to include in the sliding time window were defined by dividing each individual frequency by two. After transforming the data to the time-frequency domain, the data were decimated by a factor of two (sampling every second time point) to increase computational efficiency.

### Epoching

For both ERP and TFR analysis, the clean epoched data were separated into 8 conditions:

-   **RQ1**
    1.  *manmade*: scenes categorized as `man-made`, presented for the first time or previously (`new` and `old`), subsequently remembered or forgotten (`subsequent_remembered` and `subsequent_forgotten`), excluding `NA` in behavioral responses (although scene category is independent from response, `NA` may reflect drops in attention and, consequently, incomplete stimulus perception)
    2.  *natural*: scenes categorized as `natural`, `new` and `old`, `subsequent_remembered` and `subsequent_forgotten`, excluding `NA` in behavior
-   **RQ2**
    3.  *new*: `man-made` and `natural` scenes, presented for the first time (`new`), `subsequent_remembered` and `subsequent_forgotten`, excluding `NA` in behavior
    4.  *old*: `man-made` and `natural` scenes, presented previously (`old`), `subsequent_remembered` and `subsequent_forgotten`, excluding `NA` in behavior
-   **RQ3**
    5.  *old-hit*: `man-made` and `natural` scenes, presented previously (`old`), successfully recognized as such (`hit`), can include `NA` in memory (the image has been successfully categorized as old, regardless of whether it is recognized as such in subsequent presentations)
    6.  *old-miss*: `man-made` and `natural` scenes, presented previously (`old`), not recognized as such (`miss`), can include `NA` in memory
-   **RQ4**
    7.  *remembered*: `man-made` and `natural` scenes, `new` and `old`, `subsequent_remembered`, include all behavior
    8.  *forgotten*: `man-made` and `natural` scenes, `new` and `old`, `subsequent_forgotten`, include all behavior

# Analysis

## RQ1

The *EEGManyPipelines* team asked us to test the following:

> *RQ1. There is an effect of scene category (i.e., a difference between images showing man-made vs. natural environments) on the amplitude of the N1 component, i.e. the first major negative EEG voltage deflection.*

To address this question, we identified the N1 ERP component by visually inspecting topographies and waveforms of the grand-average signal (i.e., collapsed across all participants and both *man-made* and *natural* conditions). This unbiased *collapsed localizer* approach [@Luck2017, p. 150] revealed a negative deflection at a region of interest (*ROI*) comprising electrodes *PO7*, *PO3*, *O1*, *PO4*, *PO8*, and *O2* (see Figure \@ref(fig:figure01)), and a time window between 130 - 180 ms after stimulus onset (see Figure \@ref(fig:figure02)).\

(ref:figure01-caption) Collapsed localizer (i.e., amplitude averaged across participants and conditions between 130 - 180 ms after stimulus onset) used to identify the electrodes best representing the expected topography of the N1 ERP component (*PO7*, *PO3*, *O1*, *PO4*, *PO8*, *O2*).

```{r figure01, fig.cap = "(ref:figure01-caption)"}

knitr::include_graphics(here("results_in_repo/RQ1/topo_grand_average_ROI.png"))

```

(ref:figure02-caption) Collapsed localizer (i.e., amplitude averaged across participants and conditions at electrodes *PO7*, *PO3*, *O1*, *PO4*, *PO8*, *O2*) used to identify the time window of the N1 ERP component between 130 - 180 ms after stimulus onset (red box).

```{r figure02, fig.cap = "(ref:figure02-caption)"}

knitr::include_graphics(here("results_in_repo/RQ1/timeseries_grand_average_ROI.png"))

```

Participant- and condition-specific N1 amplitude was extracted by averaging values recorded at this ROI and time window (see Figure \@ref(fig:figure03)).\

(ref:figure03-caption) Raincloud plot [@allen2021] showing trial-averaged N1 amplitude values -- extracted from the electrode ROI and time window identified via visual inspection of the collapsed localizer --, separately for each participant and condition. Please note that trial-averaged data are shown here only for illustration purposes, whereas statistical analyses were performed on trial data.

```{r figure03, fig.cap = "(ref:figure03-caption)"}

knitr::include_graphics(here("results_in_repo/RQ1/raincloud_ERP_avg_trials_informative_prior.png"))

```

Subsequently, we fit a Bayesian multilevel linear model on N1 amplitude values, with *condition* (2 levels: *man-made*, *natural*) as **constant** (a.k.a. fixed) effect and *participant* and *trial* as **varying** (a.k.a. random) effects. We allowed intercepts and slopes to vary as a function of participant and trials, to model general and condition-specific inter-individual differences. As likelihood function, we chose a Gaussian distribution.\
An important aspect of Bayesian analysis is the choice of priors [e.g., @natarajan2000]. Given the susceptibility of the electrophysiological signal to inter-individual differences (e.g., skull thickness, skin conductance, or hair), we decided to base our priors on the current data by visually inspecting the collapsed localizer (see above). For the main analysis, we placed **informative priors** on the *intercept* -- a normal distribution with mean $\mu$ = 4 and standard deviation $\sigma$ = 2: $Normal(4,2)$ -- and *slope*, $Normal(0,1)$. To assess whether our chosen informative prior would bias parameter estimates (and, consequently, the interpretation of the results; @depaoli2017), we ran the same multilevel linear model with **weakly informative** priors (*intercept*: $Normal(4,4)$ ; *slope*: $Normal(0,4)$) and **uninformative** priors (*intercept*: $Normal(4,10)$ ; *slope*: $Normal(0,10)$). We anticipate that the choice of prior would have negligible effects on the posterior distributions, because the influence of the prior washes out with a large amount of data [@edwards1963].\
Since we had no prior knowledge regarding the standard deviation of participant and trials, we placed a **weakly informative prior** on these varying effects: a *t*-distribution with degrees of freedom $\nu$ = 3, location $\mu$ = 0, and scale $\sigma$ = 2, $Student(3,0,2)$.\
Models were fitted in *R* using the `brms` package [@bürkner2018], which employs the probabilistic programming language *Stan* [@carpenter2017] to implement a Markov chain Monte Carlo (MCMC) algorithm (i.e., No-U-Turn sampler; @homan2014) to estimate posterior distributions of the parameters of interest. Four MCMC chains with 4000 iterations (2000 warm-up) and no thinning were run to estimate parameters in each of the fitted models. Model convergence was assessed as follows: (*i*) visual inspection of trace plots, rank plots, and graphical posterior predictive checks [@gabry2019]; (*ii*) Gelman-Rubin $\hat{R}$ statistic [@gelman2013] -- comparing the between-chains variability to the within-chain variability -- between 1 and 1.05 [see also @Nalborczyk2019].\
Posterior distributions of the model parameters were summarized using the mean and 95% credible interval (CI). Differences between conditions were calculated by computing the difference between posterior distributions of the respective conditions.\
Statistical inference was performed using the **HDI + ROPE** decision rule [@kruschke2018]: posterior differences were accepted or rejected against a null hypothesis considering a small effect as "practically equivalent to zero" (Region of Practical Equivalence; *ROPE*). To mitigate the inevitable subjectivity intrinsic in arbitrarily choosing the values "practically equivalent to zero", we employed a range of plausible ROPEs, from ±0.05 $\mu$*V* to ±0.5 $\mu$*V* in steps of 0.01 $\mu$*V*. If the percentage of the posterior differences within the full ROPE was smaller than 5%, the null hypothesis was rejected [@makowski2019].

## RQ2a

The *EEGManyPipelines* team asked us to test the following:

> *RQ2a. There are effects of image novelty (i.e., between images shown for the first time/new vs. repeated/old images) within the time-range from 300--500 ms on EEG voltage at fronto-central channels.*

To address this question, for each participant and *new*/*old* condition we averaged activity in the time window between 300 - 500 ms after stimulus onset and electrode ROI *FC1*, *FC2*, *FCz*, i.e., electrodes strictly classified as fronto-central according to the international 10/10 electrode system [@chatrian1985].\

(ref:figure04-caption) Collapsed localizer (i.e., amplitude averaged across participants and conditions between 300 - 500 ms after stimulus onset) used to confirm that the selected electrode ROI (*FC1*, *FC2*, *FCz*) accurately reflects the expected negative deflection.

```{r figure04, fig.cap = "(ref:figure04-caption)"}

knitr::include_graphics(here("results_in_repo/RQ2/ERP/topo_grand_average_ROI.png"))

```

(ref:figure05-caption) Collapsed localizer (i.e., amplitude averaged across participants and conditions at electrodes *FC1*, *FC2*, *FCz*) used to identify the time window of the expected negative deflection between 300 - 500 ms after stimulus onset (red box).

```{r figure05, fig.cap = "(ref:figure05-caption)"}

knitr::include_graphics(here("results_in_repo/RQ2/ERP/timeseries_grand_average_ROI.png"))

```

Amplitude values recorded at this ROI and time window were averaged across trials, separately for each participant and condition (see Figure \@ref(fig:figure04)).\

(ref:figure06-caption) Raincloud plot showing trial-averaged amplitude values -- extracted from the specified electrode ROI and time window --, separately for each participant and condition. Please note that trial-averaged data are shown here only for illustration purposes, whereas statistical analyses were performed on trial data.

```{r figure06, fig.cap = "(ref:figure06-caption)"}

knitr::include_graphics(here("results_in_repo/RQ2/ERP/raincloud_ERP_avg_trials_informative_prior.png"))

```

We fit Bayesian multilevel linear models following the procedure described in section \@ref(rq1), with the following differences in priors: (*i*) **informative priors**: $Normal(-8,2)$ on intercept, $Normal(0,1)$ on slope; (*ii*) **weakly informative priors**: $Normal(-8,4)$ on intercept, $Normal(0,4)$ on slope; (*iii*) **uninformative priors**: $Normal(-8,10)$ on intercept, $Normal(0,10)$ on slope.\
Summaries of the posterior distributions of model parameters and statistical inference were performed as described in section \@ref(rq1).

## RQ2b and RQ2c

The *EEGManyPipelines* team asked us to test the following:

> *RQ2b. There are effects of image novelty (i.e., between images shown for the first time/new vs. repeated/old images) within the time-range from 300--500 ms on theta power at fronto-central channels.*

> *RQ2c. There are effects of image novelty (i.e., between images shown for the first time/new vs. repeated/old images) within the time-range from 300--500 ms on alpha power at posterior channels.*

To address these questions, we conducted a multilevel analysis contrasting the EEG data from trials with *old* images against trials with *new* images. At the first level (i.e., the participant level), we computed the averaged time-frequency maps for each of the two conditions. We then tested the resulting averaged maps at the second level for significant group effects using a paired-sample *t*-test. We used cluster-based permutation testing, due to its stringent control of familywise error rates [@maris2007]. Specifically, for every sample across the three channels, we quantified the experimental effect by a *t*-value. Selection of samples for inclusion in a cluster was implemented using threshold-free cluster enhancement (*TFCE*) [@smith2009]. TFCE eliminates the free parameter initial threshold value that determines which points are included in clustering by approximating a continuous integration across possible threshold values with a standard Riemann sum. We subsequently clustered selected samples in connected sets based on temporal and spectral adjacency and computed cluster-level statistics by taking the sum of the *t*-values within every cluster. Subsequently, we performed permutation testing using the Monte Carlo method (1000 permutations) to compute the significance probability of our observed effect [@maris2007] ($\alpha$ = 0.05). This analysis results in a cluster of adjacent data points across time, frequencies, and channels, which significantly differs in activity between old and new images.\
To test for differences in **theta** ($\theta$) power at fronto-central channels, we focused on the frequency range from 4 - 8 Hz and all frontocentral channels (FC1, FCz, FC2).\
To test for differences in **alpha** ($\alpha$) power at posterior channels, we focused on the frequency range from 8 - 13 Hz and all posterior channels (P7, P5, P3, P1, P2, P4, P6).

## RQ3a

The *EEGManyPipelines* team asked us to test the following:

> *RQ3a. There are effects of successful recognition of old images (i.e., a difference between old images correctly recognized as old [hits] vs. old images incorrectly judged as new [misses]) on EEG voltage at any channels, at any time.*

We followed the same analysis approach described in section \@ref(rq2b-and-rq2c) but on time-series data, comparing *old-hit*/*old-miss* conditions and including all timepoints and channels.

## RQ3b

The *EEGManyPipelines* team asked us to test the following:

> *RQ3b. There are effects of successful recognition of old images (i.e., a difference between old images correctly recognized as old [hits] vs. old images incorrectly judged as new [misses]) on spectral power, at any frequencies, at any channels, at any time.*

We followed the same analysis approach described in section \@ref(rq2b-and-rq2c), this time on *old-hit*/*old-miss* conditions and including all frequencies, timepoints, and channels.

## RQ4a

The *EEGManyPipelines* team asked us to test the following:

> *RQ4a. There are effects of subsequent memory (i.e., a difference between images that will be successfully remembered vs. forgotten on a subsequent repetition) on EEG voltage at any channels, at any time.*

We followed the same analysis approach described in section \@ref(rq3a), comparing *remembered*/*forgotten* conditions and including all timepoints and channels.

## RQ4b

The *EEGManyPipelines* team asked us to test the following:

> *RQ4b. There are effects of subsequent memory (i.e., a difference between images that will be successfully remembered vs. forgotten on a subsequent repetition) on spectral power, at any frequencies, at any channels, at any time.*

We followed the same analysis approach described in section \@ref(rq3b), this time on *remembered*/*forgotten* conditions and including all frequencies, timepoints, and channels.

# Results

## RQ1

### Descriptives and diagnostics

```{r RQ1-summary-posteriors}

RQ1_summary_posteriors_informative_prior <- 
	readRDS(
		here("results_in_repo", "RQ1", "summary_posteriors_informative_prior.rds")
		) %>% 
	as_tibble() %>% # convert to tibble
	add_column( # which prior was used?
		Prior = rep("informative", 2),
		.before = "Parameter"
	) %>% 
	mutate(
		Parameter = recode_factor(
			Parameter, # clarify labels
			"b_Intercept" = "intercept",
			"b_condition_RQ1natural" = "slope"
		)
	) %>% 
	select(-c(SD, CI, ESS))

RQ1_summary_posteriors_weaklyinformative_prior <- 
	readRDS(
		here("results_in_repo", "RQ1", "summary_posteriors_weaklyinformative_prior.rds")
	) %>% 
	as_tibble() %>%
	add_column(
		Prior = rep("weakly_informative", 2),
		.before = "Parameter"
	) %>% 
	mutate(
		Parameter = recode_factor(
			Parameter, 
			"b_Intercept" = "intercept",
			"b_condition_RQ1natural" = "slope"
		)
	) %>% 
	select(-c(SD, CI, ESS))

RQ1_summary_posteriors_noninformative_prior <- 
	readRDS(
		here("results_in_repo", "RQ1", "summary_posteriors_noninformative_prior.rds")
		) %>% 
	as_tibble() %>%
	add_column(
		Prior = rep("uninformative", 2),
		.before = "Parameter"
	) %>% 
	mutate(
		Parameter = recode_factor(
			Parameter, 
			"b_Intercept" = "intercept",
			"b_condition_RQ1natural" = "slope"
		)
	) %>% 
	select(-c(SD, CI, ESS))

```

(ref:RQ1-summary-posteriors-table-caption) **RQ1**. Means, 95% credible intervals (CI), and $\hat{R}$ statistic of the posterior distributions of intercept and slope, separately for the models with informative, weakly informative, and uninformative priors (see section \@ref(rq1)).

```{r RQ1-summary-posteriors-table}

rbind(
	RQ1_summary_posteriors_informative_prior,
	RQ1_summary_posteriors_weaklyinformative_prior,
	RQ1_summary_posteriors_noninformative_prior
) %>% 
	apa_table(
		caption = "(ref:RQ1-summary-posteriors-table-caption)",
		note = "The slope refers to manmade minus natural conditions.",
		escape = TRUE
	)

```

Table \@ref(tab:RQ1-summary-posteriors-table) shows descriptives (means and 95% CI) of the posterior distributions of intercept and slope. Model diagnostics (see $\hat{R}$ in Table \@ref(tab:RQ1-summary-posteriors-table) and Figure \@ref(fig:figure07)) confirmed that all models successfully converged.\
As anticipated in section \@ref(rq1), posterior distributions did not remarkably differ as a function of prior. Therefore, all subsequent results refer to the model with informative priors.

(ref:figure07-caption) **RQ1**. Trace plots (panel *A1*), rank plots (panel *A2*), and graphical posterior predictive checks (panel *B*) for the model with informative priors.

```{r figure07, fig.cap = "(ref:figure07-caption)"}

knitr::include_graphics(here("results_in_repo/RQ1/model_diagnostics_informative_prior.png"))

```

### Hypothesis testing

```{r RQ1-equivalence-test}

RQ1_equivalence_test_informative_prior <- 
	readRDS(
		here("results_in_repo", "RQ1", "equivalence_test_informative_prior.rds")
		)

RQ1_max_ROPE <- 
	RQ1_equivalence_test_informative_prior %>%
	filter(ROPE_Equivalence == "Rejected") %>% # keep values for which H0 is rejected
	pull(ROPE_high) %>% # extract vector
	max() # find largest value

```

The results of the **HDI + ROPE** procedure showed that 95% of the posterior distribution of the N1 amplitude difference between *manmade* and *natural* scenes was outside of a region of practical equivalence up until ±`r RQ1_max_ROPE` $\mu$*V*. In other words, ***manmade*** **scenes elicited an N1 whose amplitude was at most `r RQ1_max_ROPE`** $\mu$*V* larger than the N1 elicited by *natural* scenes (see Figure \@ref(fig:figure08)).

(ref:figure08-caption) Panel *A* shows the posterior distributions of *manmade* and *natural* conditions. Panel *B* shows the posterior distribution of their difference (*manmade* minus *natural*), as well as the largest ROPE (±`r RQ1_max_ROPE` $\mu$*V*) for which the null hypothesis was rejected by using the *HDI + ROPE* procedure described in section \@ref(rq1).

```{r figure08, fig.cap = "(ref:figure08-caption)"}

knitr::include_graphics(here("results_in_repo/RQ1/posterior_distributions_informative_prior.png"))

```

## RQ2a

### Descriptives and diagnostics

```{r RQ2a-summary-posteriors}

RQ2a_summary_posteriors_informative_prior <- 
	readRDS(
		here("results_in_repo", "RQ2", "ERP", "summary_posteriors_informative_prior.rds")
		) %>% 
	as_tibble() %>%
	add_column(
		Prior = rep("informative", 2),
		.before = "Parameter"
	) %>% 
	mutate(
		Parameter = recode_factor(
			Parameter,
			"b_Intercept" = "intercept",
			"b_condition_RQ2old" = "slope"
		)
	) %>% 
	select(-c(SD, CI, ESS))

RQ2a_summary_posteriors_weaklyinformative_prior <- 
	readRDS(
		here("results_in_repo", "RQ2", "ERP", "summary_posteriors_weaklyinformative_prior.rds")
	) %>% 
	as_tibble() %>%
	add_column(
		Prior = rep("weakly_informative", 2),
		.before = "Parameter"
	) %>% 
	mutate(
		Parameter = recode_factor(
			Parameter, 
			"b_Intercept" = "intercept",
			"b_condition_RQ2old" = "slope"
		)
	) %>% 
	select(-c(SD, CI, ESS))

RQ2a_summary_posteriors_noninformative_prior <- 
	readRDS(
		here("results_in_repo", "RQ2", "ERP", "summary_posteriors_noninformative_prior.rds")
		) %>% 
	as_tibble() %>%
	add_column(
		Prior = rep("uninformative", 2),
		.before = "Parameter"
	) %>% 
	mutate(
		Parameter = recode_factor(
			Parameter, 
			"b_Intercept" = "intercept",
			"b_condition_RQ2old" = "slope"
		)
	) %>% 
	select(-c(SD, CI, ESS))

```

(ref:RQ2a-summary-posteriors-table-caption) **RQ2a**. Means, 95% credible intervals (CI), and $\hat{R}$ statistic of the posterior distributions of intercept and slope, separately for the models with informative, weakly informative, and uninformative priors (see section \@ref(rq1)).

```{r RQ2a-summary-posteriors-table}

rbind(
	RQ2a_summary_posteriors_informative_prior,
	RQ2a_summary_posteriors_weaklyinformative_prior,
	RQ2a_summary_posteriors_noninformative_prior
) %>% 
	apa_table(
		caption = "(ref:RQ2a-summary-posteriors-table-caption)",
		note = "The slope refers to manmade minus natural conditions.",
		escape = TRUE
	)

```

Table \@ref(tab:RQ2a-summary-posteriors-table) shows descriptives (means and 95% CI) of the posterior distributions of intercept and slope. Model diagnostics (see $\hat{R}$ in Table \@ref(tab:RQ2a-summary-posteriors-table) and Figure \@ref(fig:figure09)) confirmed that all models successfully converged.\
As anticipated in section \@ref(rq1), posterior distributions did not remarkably differ as a function of prior. Therefore, all subsequent results refer to the model with informative priors.

(ref:figure09-caption) **RQ2a**. Trace plots (panel *A1*), rank plots (panel *A2*), and graphical posterior predictive checks (panel *B*) for the model with informative priors.

```{r figure09, fig.cap = "(ref:figure09-caption)"}

knitr::include_graphics(here("results_in_repo/RQ2/ERP/model_diagnostics_informative_prior.png"))

```

### Hypothesis testing

```{r RQ2a-equivalence-test}

RQ2a_equivalence_test_informative_prior <- 
	readRDS(
		here("results_in_repo", "RQ2", "ERP", "equivalence_test_informative_prior.rds")
		)

RQ2a_max_ROPE <- 
	RQ2a_equivalence_test_informative_prior %>%
	filter(ROPE_Equivalence == "Rejected") %>%
	pull(ROPE_high) %>%
	max()

```

The results of the **HDI + ROPE** procedure showed that 95% of the posterior distribution of the amplitude difference between *old* and *new* scenes was outside of a region of practical equivalence up until ±`r RQ2a_max_ROPE` $\mu$*V*. In other words, ***new*** **scenes elicited electrophysiological brain activity whose amplitude was at most `r RQ2a_max_ROPE`** $\mu$*V* more negative than the activity elicited by *old* scenes (see Figure \@ref(fig:figure10)).

(ref:figure10-caption) **RQ2a**. Panel *A* shows the posterior distributions of *new* and *old* conditions. Panel *B* shows the posterior distribution of their difference (*new* minus *old*), as well as the largest ROPE (±`r RQ2a_max_ROPE` $\mu$*V*) for which the null hypothesis was rejected by using the *HDI + ROPE* procedure described in section \@ref(rq1).

```{r figure10, fig.cap = "(ref:figure10-caption)"}

knitr::include_graphics(here("results_in_repo/RQ2/ERP/posterior_distributions_informative_prior.png"))

```

## RQ2b, RQ2c, RQ3a, RQ3b, RQ4a, RQ4b

***No statistically significant clusters*** were identified for any of these research questions, ***at any time point, electrode, or frequency***.

# Reproducibility

To increase the likelihood of numerical reproducibility and ensure that all the data processing and analysis steps were executed in the correct sequence, we used GNU Make [@MakeBook] to manage the workflow for this project. For an introduction to GNU Make and basic concepts related to using a `Makefile`, we recommend sections 1-3 of [@JSSMake].

The `makefile` for this project has two targets specified in `all`: `initial_setup` and `analysis`. `initial_setup` is built before any processing and analysis, and does the following three steps:

1.  create the `tmp` and `zzz_receipts` directories

The `tmp` directory is used to store files that are not added to the repository, but that are necessary for processing data and doing the analysis. For example, one of the files stores information about machine-specific paths to directories outside the repository where large files are stored (i.e., `eeg_BIDS`).

2.  restore file timestamps, such that outdated targets are correctly detected and built

Because some of the files are too large to be added to the GitHub hosted repository, we are saving receipts (in `zzz_receipts`) to keep track of which targets need to be build. When cloning the repository, the timestamp of the files within the repo corresponds to the time when they were cloned on the device. In addition, files are cloned in alphabetic order. This means that `filename_A` will be downloaded before `filename_B` and as a result, the Makefile will see `filename_B` as more recent than `filename_A`. This leads to unnecessarely rebuilding targets, which is an issue if those steps are computationally intensive. To address this issue, `scripts/restore_file_timestamps.sh` changes the timestamp of all files in the repository to match the time of the most recent commit (instead of the time when they were cloned).

Note that this approach can be risky and lead to issues. To avoid problems, the user needs to always build all targets (type `make` in the terminal) before commiting and pushing to the repository. By doing so, all outdated targets are build before commit and push, which means that the date of the commit is informative of when the target was last built.

3.  test the `.Rmd` and `reticulate` combination

The preprocessing steps and part of the analysis scripts are using `python`. If `python` scripts are executed in the terminal, then the output that is printed in the terminal cannot be inspected at a later time because it is not saved on the machine executing the script. To keep records of this output, we are executing `python` from within an `.Rmd` file, making use of `reticulate` tools. This setup is relatively more complex, and it is advisable that it is tested before trying to build any of the analysis targets. `initial_setup` tests if `python` can be executed from within R. If the test is successful, then `test_make.html` is generated in `tmp/`.

The `analysis` targets contains four sub-targets, one for each RQ. These targets have as prerequisites the preprocessing and analysis steps.

To build the targets specified in `all`, type `make` in the terminal. This will first build `initial_setup` and then `analysis`. Before building the targets, the user needs to specify the path to a local directory (`DIR_local_files` in the `makefile`), outside of the repository, that contains the raw data: `data_outside_repo/original_data/eeg_BIDS`. `eeg_BIDS` has the same structure and content as the directory provided on October 8th 2021. As the targets are built, other directories are created within `DIR_local_files`, where large files are stored.

*Important note*: building all targets is time consuming and computaitonally intensive (e.g., up to 7-10 days on a Windows machine with 64GB of RAM and Intel Xeon CPU E5-1620 V3 3.5Hz). See `report/all_Make_steps.txt` for a print of all the targets that are built, the order in which each of the scripts are executed, and the arguments provided for each script.

## Software

Information about the software used for this project is available in `report/Software_info.html`. The corresponding `report/Software_info.Rmd` was knit on the device that was used to build all the targets in the `makefile`. Specifically:

-   **version control**: `git` *v*2.31.1.windows.1

-   **automation**: `GNU Make` *v*4.3

-   **R, Python, and the interface between them**: *R* *v*4.2.0 [@R-base], *RStudio IDE* *v*2022.02.2+485 [@R-studio], *Python* *v*3.8.13 [@vanrossum-2009], `reticulate` *v*1.24 [@reticulate].

`python` packages were installed using `reticulate::py_install("name_of_package")`. `R` packages available on CRAN were installed using `install.packages("name_of_package")` and `R` packages available only at GitHub were installed using the `install_github` function in `remotes` *v*2.4.2 [@remotes].

-   **Python packages**: `mne` *v*1.0.2 [@gramfort2013], `autoreject` *v*0.3.1 [@jas2017], `mne_bids` *v*0.10 [@Appelhoff2019] [@pernet2019eeg], `numpy` *v*1.21.6 [@harris2020array], `pandas` *v*1.4.2 [@reback2022pandas], `pyprep` *v*0.4.2 [@appelhoff2022].

-   **R packages**:

    -   **data wrangling and analysis**: `here` *v*1.0.1 [@here], `Rmisc` *v*1.5.1 [@Rmisc], `tidyverse` *v*1.3.1 [@tidyverse] -- in particular `tibble` *v*3.1.7 [@tibble], `tidyr` *v*1.2.0 [@tidyr], `readr` *v*2.1.2 [@readr], `dplyr` *v*1.0.9 [@dplyr] --, `brms` *v*2.17.0 [@bürkner2018], `eegUtils` *v*0.7.0 [@eegUtils], `emmeans` *v*1.7.3 [@emmeans], `bayestestR` *v*0.12.1 [@bayestestR], `tools` *v*4.2.0 [@R-base]

    -   **visualization**: `ggplot2` *v*3.3.6 [@ggplot2], `eegUtils` *v*0.7.0 [@eegUtils], `bayesplot` *v*1.9.0 [@bayesplot], `viridis` *v*0.6.2 [@viridis], `tidybayes` *v*3.0.2 [@tidybayes], `patchwork` *v*1.1.1 [@patchwork]

    -   **report generation**: `knitr` *v*1.39 [@knitr], `rmarkdown` *v*2.14 [@rmarkdown], `papaja` *v*0.1.0.9999 [@papaja]

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
