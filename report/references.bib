@Article{gramfortMEGEEGData2013,
  title = {{{MEG}} and {{EEG}} Data Analysis with {{MNE-Python}}},
  author = {Alexandre Gramfort and Martin Luessi and Eric Larson and Denis A. Engemann and Daniel Strohmeier and Christian Brodbeck and Roman Goj and Mainak Jas and Teon Brooks and Lauri Parkkonen and Matti H{\"a}m{\"a}l{\"a}inen},
  date = {2013-12-26},
  journaltitle = {Frontiers in Neuroscience},
  shortjournal = {Front Neurosci},
  volume = {7},
  eprint = {24431986},
  eprinttype = {pmid},
  pages = {267},
  issn = {1662-4548},
  doi = {10.3389/fnins.2013.00267},
  abstract = {Magnetoencephalography and electroencephalography (M/EEG) measure the weak electromagnetic signals generated by neuronal activity in the brain. Using these signals to characterize and locate neural activation in the brain is a challenge that requires expertise in physics, signal processing, statistics, and numerical methods. As part of the MNE software suite, MNE-Python is an open-source software package that addresses this challenge by providing state-of-the-art algorithms implemented in Python that cover multiple methods of data preprocessing, source localization, statistical analysis, and estimation of functional connectivity between distributed brain regions. All algorithms and utility functions are implemented in a consistent manner with well-documented interfaces, enabling users to create M/EEG data analysis pipelines by writing Python scripts. Moreover, MNE-Python is tightly integrated with the core Python libraries for scientific comptutation (NumPy, SciPy) and visualization (matplotlib and Mayavi), as well as the greater neuroimaging ecosystem in Python via the Nibabel package. The code is provided under the new BSD license allowing code reuse, even in commercial products. Although MNE-Python has only been under heavy development for a couple of years, it has rapidly evolved with expanded analysis capabilities and pedagogical tutorials because multiple labs have collaborated during code development to help share best practices. MNE-Python also gives easy access to preprocessed datasets, helping users to get started quickly and facilitating reproducibility of methods by other researchers. Full documentation, including dozens of examples, is available at http://martinos.org/mne.},
  langid = {english},
  pmcid = {PMC3872725},
  keywords = {electroencephalography (EEG),magnetoencephalography (MEG),neuroimaging,open-source,python,software},
}
@Article{bigdely-shamloPREPPipelineStandardized2015,
  title = {The {{PREP}} Pipeline: Standardized Preprocessing for Large-Scale {{EEG}} Analysis},
  shorttitle = {The {{PREP}} Pipeline},
  author = {Nima Bigdely-Shamlo and Tim Mullen and Christian Kothe and Kyung-Min Su and Kay A. Robbins},
  date = {2015},
  journaltitle = {Frontiers in Neuroinformatics},
  shortjournal = {Front Neuroinform},
  volume = {9},
  eprint = {26150785},
  eprinttype = {pmid},
  pages = {16},
  issn = {1662-5196},
  doi = {10.3389/fninf.2015.00016},
  abstract = {The technology to collect brain imaging and physiological measures has become portable and ubiquitous, opening the possibility of large-scale analysis of real-world human imaging. By its nature, such data is large and complex, making automated processing essential. This paper shows how lack of attention to the very early stages of an EEG preprocessing pipeline can reduce the signal-to-noise ratio and introduce unwanted artifacts into the data, particularly for computations done in single precision. We demonstrate that ordinary average referencing improves the signal-to-noise ratio, but that noisy channels can contaminate the results. We also show that identification of noisy channels depends on the reference and examine the complex interaction of filtering, noisy channel identification, and referencing. We introduce a multi-stage robust referencing scheme to deal with the noisy channel-reference interaction. We propose a standardized early-stage EEG processing pipeline (PREP) and discuss the application of the pipeline to more than 600 EEG datasets. The pipeline includes an automatically generated report for each dataset processed. Users can download the PREP pipeline as a freely available MATLAB library from http://eegstudy.org/prepcode.},
  langid = {english},
  pmcid = {PMC4471356},
  keywords = {artifact,BCILAB,big data,EEG,EEGLAB,machine learning,preprocessing},
}
@Article{perrinSphericalSplinesScalp1989,
  title = {Spherical Splines for Scalp Potential and Current Density Mapping},
  author = {F. Perrin and J. Pernier and O. Bertrand and J. F. Echallier},
  date = {1989-02},
  journaltitle = {Electroencephalography and Clinical Neurophysiology},
  shortjournal = {Electroencephalogr Clin Neurophysiol},
  volume = {72},
  number = {2},
  eprint = {2464490},
  eprinttype = {pmid},
  pages = {184--187},
  issn = {0013-4694},
  doi = {10.1016/0013-4694(89)90180-6},
  abstract = {Description of mapping methods using spherical splines, both to interpolate scalp potentials (SPs), and to approximate scalp current densities (SCDs). Compared to a previously published method using thin plate splines, the advantages are a very simple derivation of the SCD approximation, faster computing times, and greater accuracy in areas with few electrodes.},
  langid = {english},
  keywords = {Computer Simulation,Electroencephalography,Electrophysiology,Humans,Scalp,Signal Processing; Computer-Assisted},
}
@Article{jasAutorejectAutomatedArtifact2017,
  title = {Autoreject: {{Automated}} Artifact Rejection for {{MEG}} and {{EEG}} Data},
  shorttitle = {Autoreject},
  author = {Mainak Jas and Denis A. Engemann and Yousra Bekhti and Federico Raimondo and Alexandre Gramfort},
  date = {2017-10-01},
  journaltitle = {NeuroImage},
  shortjournal = {Neuroimage},
  volume = {159},
  eprint = {28645840},
  eprinttype = {pmid},
  pages = {417--429},
  issn = {1095-9572},
  doi = {10.1016/j.neuroimage.2017.06.030},
  abstract = {We present an automated algorithm for unified rejection and repair of bad trials in magnetoencephalography (MEG) and electroencephalography (EEG) signals. Our method capitalizes on cross-validation in conjunction with a robust evaluation metric to estimate the optimal peak-to-peak threshold - a quantity commonly used for identifying bad trials in M/EEG. This approach is then extended to a more sophisticated algorithm which estimates this threshold for each sensor yielding trial-wise bad sensors. Depending on the number of bad sensors, the trial is then repaired by interpolation or by excluding it from subsequent analysis. All steps of the algorithm are fully automated thus lending itself to the name Autoreject. In order to assess the practical significance of the algorithm, we conducted extensive validation and comparisons with state-of-the-art methods on four public datasets containing MEG and EEG recordings from more than 200 subjects. The comparisons include purely qualitative efforts as well as quantitatively benchmarking against human supervised and semi-automated preprocessing pipelines. The algorithm allowed us to automate the preprocessing of MEG data from the Human Connectome Project (HCP) going up to the computation of the evoked responses. The automated nature of our method minimizes the burden of human inspection, hence supporting scalability and reliability demanded by data analysis in modern neuroscience.},
  langid = {english},
  pmcid = {PMC7243972},
  keywords = {Algorithms,Artifacts,Automated analysis,Brain,Brain Mapping,Cross-validation,Electroencephalogram (EEG),Electroencephalography,Human Connectome Project (HCP),Humans,Magnetoencephalography,Magnetoencephalography (MEG),Models; Neurological,Preprocessing,Signal Processing; Computer-Assisted,Statistical learning},
}
@Article{smithThresholdfreeClusterEnhancement2009,
  title = {Threshold-Free Cluster Enhancement: Addressing Problems of Smoothing, Threshold Dependence and Localisation in Cluster Inference},
  shorttitle = {Threshold-Free Cluster Enhancement},
  author = {Stephen M. Smith and Thomas E. Nichols},
  date = {2009-01-01},
  journaltitle = {NeuroImage},
  shortjournal = {Neuroimage},
  volume = {44},
  number = {1},
  eprint = {18501637},
  eprinttype = {pmid},
  pages = {83--98},
  issn = {1095-9572},
  doi = {10.1016/j.neuroimage.2008.03.061},
  abstract = {Many image enhancement and thresholding techniques make use of spatial neighbourhood information to boost belief in extended areas of signal. The most common such approach in neuroimaging is cluster-based thresholding, which is often more sensitive than voxel-wise thresholding. However, a limitation is the need to define the initial cluster-forming threshold. This threshold is arbitrary, and yet its exact choice can have a large impact on the results, particularly at the lower (e.g., t, z {$<$} 4) cluster-forming thresholds frequently used. Furthermore, the amount of spatial pre-smoothing is also arbitrary (given that the expected signal extent is very rarely known in advance of the analysis). In the light of such problems, we propose a new method which attempts to keep the sensitivity benefits of cluster-based thresholding (and indeed the general concept of {"}clusters{"} of signal), while avoiding (or at least minimising) these problems. The method takes a raw statistic image and produces an output image in which the voxel-wise values represent the amount of cluster-like local spatial support. The method is thus referred to as {"}threshold-free cluster enhancement{"} (TFCE). We present the TFCE approach and discuss in detail ROC-based optimisation and comparisons with cluster-based and voxel-based thresholding. We find that TFCE gives generally better sensitivity than other methods over a wide range of test signal shapes and SNR values. We also show an example on a real imaging dataset, suggesting that TFCE does indeed provide not just improved sensitivity, but richer and more interpretable output than cluster-based thresholding.},
  langid = {english},
  keywords = {Algorithms,Area Under Curve,Brain Mapping,Humans,Image Enhancement,Image Processing; Computer-Assisted,ROC Curve},
}
@Article{marisNonparametricStatisticalTesting2007,
  title = {Nonparametric Statistical Testing of {{EEG-}} and {{MEG-data}}},
  author = {Eric Maris and Robert Oostenveld},
  date = {2007-08-15},
  journaltitle = {Journal of Neuroscience Methods},
  shortjournal = {J Neurosci Methods},
  volume = {164},
  number = {1},
  eprint = {17517438},
  eprinttype = {pmid},
  pages = {177--190},
  issn = {0165-0270},
  doi = {10.1016/j.jneumeth.2007.03.024},
  abstract = {In this paper, we show how ElectroEncephaloGraphic (EEG) and MagnetoEncephaloGraphic (MEG) data can be analyzed statistically using nonparametric techniques. Nonparametric statistical tests offer complete freedom to the user with respect to the test statistic by means of which the experimental conditions are compared. This freedom provides a straightforward way to solve the multiple comparisons problem (MCP) and it allows to incorporate biophysically motivated constraints in the test statistic, which may drastically increase the sensitivity of the statistical test. The paper is written for two audiences: (1) empirical neuroscientists looking for the most appropriate data analysis method, and (2) methodologists interested in the theoretical concepts behind nonparametric statistical tests. For the empirical neuroscientist, a large part of the paper is written in a tutorial-like fashion, enabling neuroscientists to construct their own statistical test, maximizing the sensitivity to the expected effect. And for the methodologist, it is explained why the nonparametric test is formally correct. This means that we formulate a null hypothesis (identical probability distribution in the different experimental conditions) and show that the nonparametric test controls the false alarm rate under this null hypothesis.},
  langid = {english},
  keywords = {Brain,Brain Mapping,Data Interpretation; Statistical,Electroencephalography,Evoked Potentials,Humans,Magnetoencephalography,Signal Processing; Computer-Assisted,Statistics; Nonparametric},
}
@Article{depaoliImprovingTransparencyReplication2017,
  title = {Improving Transparency and Replication in {{Bayesian}} Statistics: {{The WAMBS-Checklist}}.},
  shorttitle = {Improving Transparency and Replication in {{Bayesian}} Statistics},
  author = {Sarah Depaoli and Rens {van de Schoot}},
  options = {useprefix=true},
  date = {2017-06},
  journaltitle = {Psychological Methods},
  shortjournal = {Psychological Methods},
  volume = {22},
  number = {2},
  pages = {240--261},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000065},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000065},
  urldate = {2022-04-27},
  langid = {english},
}
@Article{luckHowGetStatistically2017,
  title = {How to Get Statistically Significant Effects in Any {{ERP}} Experiment (and Why You Shouldn't): {{How}} to Get Significant Effects},
  shorttitle = {How to Get Statistically Significant Effects in Any {{ERP}} Experiment (and Why You Shouldn't)},
  author = {Steven J. Luck and Nicholas Gaspelin},
  date = {2017-01},
  journaltitle = {Psychophysiology},
  shortjournal = {Psychophysiol},
  volume = {54},
  number = {1},
  pages = {146--157},
  issn = {00485772},
  doi = {10.1111/psyp.12639},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/psyp.12639},
  urldate = {2022-04-27},
  langid = {english},
}
@Article{edwardsBayesianStatisticalInference1963,
  title = {Bayesian Statistical Inference for Psychological Research.},
  author = {Ward Edwards and Harold Lindman and Leonard J. Savage},
  date = {1963-05},
  journaltitle = {Psychological Review},
  shortjournal = {Psychological Review},
  volume = {70},
  number = {3},
  pages = {193--242},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/h0044139},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0044139},
  urldate = {2022-04-27},
  langid = {english},
}
@Article{natarajanReferenceBayesianMethods2000,
  title = {Reference {{Bayesian Methods}} for {{Generalized Linear Mixed Models}}},
  author = {Ranjini Natarajan and Robert E. Kass},
  date = {2000-03},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  volume = {95},
  number = {449},
  pages = {227--237},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2000.10473916},
  url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.2000.10473916},
  urldate = {2022-04-27},
  langid = {english},
}
@Article{burknerAdvancedBayesianMultilevel2018,
  title = {Advanced {{Bayesian Multilevel Modeling}} with the {{R Package}} Brms},
  author = {Paul-Christian B{\"u}rkner},
  date = {2018},
  journaltitle = {The R Journal},
  volume = {10},
  number = {1},
  pages = {395--411},
  issn = {2073-4859},
  url = {https://journal.r-project.org/archive/2018/RJ-2018-017/index.html},
  urldate = {2022-04-27},
  langid = {english},
}
@Article{carpenterStanProbabilisticProgramming2017,
  title = {Stan: {{A Probabilistic Programming Language}}},
  shorttitle = {Stan},
  author = {Bob Carpenter and Andrew Gelman and Matthew D. Hoffman and Daniel Lee and Ben Goodrich and Michael Betancourt and Marcus Brubaker and Jiqiang Guo and Peter Li and Allen Riddell},
  date = {2017-01-11},
  journaltitle = {Journal of Statistical Software},
  volume = {76},
  pages = {1--32},
  issn = {1548-7660},
  doi = {10.18637/jss.v076.i01},
  url = {https://doi.org/10.18637/jss.v076.i01},
  urldate = {2022-04-27},
  abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.},
  langid = {english},
  keywords = {Stan},
}
@Article{homanNoUturnSamplerAdaptively2014,
  title = {The {{No-U-turn}} Sampler: Adaptively Setting Path Lengths in {{Hamiltonian Monte Carlo}}},
  shorttitle = {The {{No-U-turn}} Sampler},
  author = {Matthew D. Homan and Andrew Gelman},
  date = {2014-01-01},
  journaltitle = {The Journal of Machine Learning Research},
  shortjournal = {J. Mach. Learn. Res.},
  volume = {15},
  number = {1},
  pages = {1593--1623},
  issn = {1532-4435},
  abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size ε and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS performs at least as efficiently as (and sometimes more effciently than) a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter ε on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all, making it suitable for applications such as BUGS-style automatic inference engines that require efficient {"}turnkey{"} samplers.},
  keywords = {adaptive Monte Carlo,Bayesian inference,dual averaging,Hamiltonian Monte Carlo,Markov chain Monte Carlo},
}
@Article{nalborczykIntroductionBayesianMultilevel2019,
  title = {An {{Introduction}} to {{Bayesian Multilevel Models Using}} Brms: {{A Case Study}} of {{Gender Effects}} on {{Vowel Variability}} in {{Standard Indonesian}}},
  shorttitle = {An {{Introduction}} to {{Bayesian Multilevel Models Using}} Brms},
  author = {Ladislas Nalborczyk and C{\a'e}dric Batailler and H{\a'e}l{\a`e}ne L{\oe}venbruck and Anne Vilain and Paul-Christian B{\"u}rkner},
  date = {2019-05-21},
  journaltitle = {Journal of speech, language, and hearing research: JSLHR},
  shortjournal = {J Speech Lang Hear Res},
  volume = {62},
  number = {5},
  eprint = {31082309},
  eprinttype = {pmid},
  pages = {1225--1242},
  issn = {1558-9102},
  doi = {10.1044/2018_JSLHR-S-18-0006},
  abstract = {Purpose Bayesian multilevel models are increasingly used to overcome the limitations of frequentist approaches in the analysis of complex structured data. This tutorial introduces Bayesian multilevel modeling for the specific analysis of speech data, using the brms package developed in R. Method In this tutorial, we provide a practical introduction to Bayesian multilevel modeling by reanalyzing a phonetic data set containing formant (F1 and F2) values for 5 vowels of standard Indonesian (ISO 639-3:ind), as spoken by 8 speakers (4 females and 4 males), with several repetitions of each vowel. Results We first give an introductory overview of the Bayesian framework and multilevel modeling. We then show how Bayesian multilevel models can be fitted using the probabilistic programming language Stan and the R package brms, which provides an intuitive formula syntax. Conclusions Through this tutorial, we demonstrate some of the advantages of the Bayesian framework for statistical modeling and provide a detailed case study, with complete source code for full reproducibility of the analyses ( https://osf.io/dpzcb /). Supplemental Material https://doi.org/10.23641/asha.7973822.},
  langid = {english},
  keywords = {Bayes Theorem,Female,Humans,Indonesia,Language,Male,Multilevel Analysis,Phonation,Phonetics,Sex Characteristics,Speech},
}
@Article{gabryVisualizationBayesianWorkflow2019,
  title = {Visualization in {{Bayesian}} Workflow},
  author = {Jonah Gabry and Daniel Simpson and Aki Vehtari and Michael Betancourt and Andrew Gelman},
  date = {2019-02},
  journaltitle = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  shortjournal = {J. R. Stat. Soc. A},
  volume = {182},
  number = {2},
  pages = {389--402},
  issn = {0964-1998, 1467-985X},
  doi = {10.1111/rssa.12378},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/rssa.12378},
  urldate = {2022-04-27},
  langid = {english},
}
@Book{gelmanBayesianDataAnalysis2013,
  title = {Bayesian {{Data Analysis}}},
  author = {A. Gelman and J.B. Carlin and H.S. Stern and D.B. Dunson and A. Vehtari and D.B. Rubin},
  date = {2013},
  edition = {3rd},
  publisher = {{CRC Press}},
}
@Book{kruschkeDoingBayesianData2014,
  title = {Doing {{Bayesian Data Analysis}}: {{A Tutorial}} with {{R}}, {{JAGS}}, and {{Stan}}},
  shorttitle = {Doing {{Bayesian Data Analysis}}},
  author = {J.K. Kruschke},
  date = {2014-11-17},
  edition = {2nd edition},
  publisher = {{Academic Press}},
  location = {{Boston}},
  abstract = {Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan, Second Edition provides an accessible approach for conducting Bayesian data analysis, as material is explained clearly with concrete examples. Included are step-by-step instructions on how to carry out Bayesian data analyses in the popular and free software R and WinBugs, as well as new programs in JAGS and Stan. The new programs are designed to be much easier to use than the scripts in the first edition. In particular, there are now compact high-level scripts that make it easy to run the programs on your own data sets. The book is divided into three parts and begins with the basics: models, probability, Bayes’ rule, and the R programming language. The discussion then moves to the fundamentals applied to inferring a binomial probability, before concluding with chapters on the generalized linear model. Topics include metric-predicted variable on one or two groups; metric-predicted variable with one metric predictor; metric-predicted variable with multiple metric predictors; metric-predicted variable with one nominal predictor; and metric-predicted variable with multiple nominal predictors. The exercises found in the text have explicit purposes and guidelines for accomplishment. This book is intended for first-year graduate students or advanced undergraduates in statistics, data analysis, psychology, cognitive science, social sciences, clinical sciences, and consumer sciences in business. Accessible, including the basics of essential concepts of probability and random samplingExamples with R programming language and JAGS softwareComprehensive coverage of all scenarios addressed by non-Bayesian textbooks: t-tests, analysis of variance (ANOVA) and comparisons in ANOVA, multiple regression, and chi-square (contingency table analysis)Coverage of experiment planningR and JAGS computer programming code on websiteExercises have explicit purposes and guidelines for accomplishmentProvides step-by-step instructions on how to conduct Bayesian data analyses in the popular and free software R and WinBugs},
  isbn = {978-0-12-405888-0},
  langid = {english},
  pagetotal = {776},
}
@Article{kruschkeRejectingAcceptingParameter2018,
  title = {Rejecting or {{Accepting Parameter Values}} in {{Bayesian Estimation}}},
  author = {John K. Kruschke},
  date = {2018-06},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {2},
  pages = {270--280},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/2515245918771304},
  url = {http://journals.sagepub.com/doi/10.1177/2515245918771304},
  urldate = {2022-04-27},
  abstract = {This article explains a decision rule that uses Bayesian posterior distributions as the basis for accepting or rejecting null values of parameters. This decision rule focuses on the range of plausible values indicated by the highest density interval of the posterior distribution and the relation between this range and a region of practical equivalence (ROPE) around the null value. The article also discusses considerations for setting the limits of a ROPE and emphasizes that analogous considerations apply to setting the decision thresholds for p values and Bayes factors.},
  langid = {english},
}
@Book{10.5555/1593511,
  title = {Python 3 Reference Manual},
  author = {Guido {Van Rossum} and Fred L. Drake},
  date = {2009},
  publisher = {{CreateSpace}},
  location = {{Scotts Valley, CA}},
  isbn = {1-4414-1269-7},
}
@Article{raybaut2009spyder,
  title = {Spyder-Documentation},
  author = {Pierre Raybaut},
  date = {2009},
  journaltitle = {Available online at: pythonhosted. org},
}
@Manual{R-base,
  type = {manual},
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  date = {2022},
  location = {{Vienna, Austria}},
  url = {https://www.R-project.org/},
  organization = {{R Foundation for Statistical Computing}},
}
@Manual{R-studio,
  type = {manual},
  title = {{{RStudio}}: {{Integrated}} Development Environment for {{R}}},
  author = {{RStudio Team}},
  date = {2020},
  location = {{Boston, MA}},
  url = {http://www.rstudio.com/},
  organization = {{RStudio, PBC.}},
}
